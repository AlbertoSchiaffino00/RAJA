/*!
 ******************************************************************************
 *
 * \file
 *
 * \brief   RAJA header file defining SIMD/SIMT register operations.
 *
 ******************************************************************************
 */

//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//
// Copyright (c) 2016-19, Lawrence Livermore National Security, LLC
// and RAJA project contributors. See the RAJA/COPYRIGHT file for details.
//
// SPDX-License-Identifier: (BSD-3-Clause)
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//

#ifndef RAJA_pattern_tensor_ET_TensorMultiplyAddAdd_HPP
#define RAJA_pattern_tensor_ET_TensorMultiplyAddAdd_HPP

#include "RAJA/config.hpp"

#include "RAJA/util/macros.hpp"

#include "RAJA/pattern/tensor/internal/ET/ExpressionTemplateBase.hpp"
#include "RAJA/pattern/tensor/internal/ET/DefaultMultiply.hpp"

namespace RAJA
{

  namespace internal
  {

  namespace ET
  {


    /*!
     * Expression for LHS*RHS+ADD, which allows for accessing FMA style
     * operations.
     *
     * This ET can only be generated by contracting an Add and Multiple ET.
     *
     */
    template<typename LHS_TYPE, typename RHS_TYPE, typename ADD_TYPE>
    class TensorMultiplyAdd : public TensorExpressionBase<TensorMultiplyAdd<LHS_TYPE, RHS_TYPE, ADD_TYPE>> {
      public:
        using self_type = TensorMultiplyAdd<LHS_TYPE, RHS_TYPE, ADD_TYPE>;
        using lhs_type = LHS_TYPE;
        using rhs_type = RHS_TYPE;
        using add_type = ADD_TYPE;
        using element_type = typename LHS_TYPE::element_type;
        using index_type = typename LHS_TYPE::index_type;
        using tile_type = typename LHS_TYPE::tile_type;


        // Note that the ADD_TYPE is both the result of lhs*rhs and the
        // return type
        using result_type = typename ADD_TYPE::result_type;

        using default_multiply = DefaultMultiply<LHS_TYPE, RHS_TYPE>;


        static constexpr camp::idx_t s_num_dims = result_type::s_num_dims;

        RAJA_INLINE
        RAJA_HOST_DEVICE
        TensorMultiplyAdd(lhs_type const &lhs, rhs_type const &rhs,
                          add_type const &add) :
        m_lhs{lhs}, m_rhs{rhs}, m_add{add}
        {}


        template<typename TILE_TYPE>
        RAJA_INLINE
        RAJA_HOST_DEVICE
        result_type eval(TILE_TYPE const &tile) const {
          return default_multiply::multiply_add(tile, m_lhs, m_rhs, m_add.eval(tile));
        }


        RAJA_INLINE
        RAJA_HOST_DEVICE
        void print_ast() const {
          printf("MultiplyAdd[");
          default_multiply::print_ast();
          printf("](");
          m_lhs.print_ast();
          printf(", ");
          m_rhs.print_ast();
          printf(", ");
          m_add.print_ast();
          printf(")");
        }


      private:
        lhs_type m_lhs;
        rhs_type m_rhs;
        add_type m_add;
    };




  } // namespace ET

  } // namespace internal

}  // namespace RAJA


#endif
